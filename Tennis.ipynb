{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition Project 3\n",
    "\n",
    "---\n",
    "\n",
    "Third hands-on project of the Deep Reinforcement Learning Nanodegree.\n",
    "\n",
    "Note: as mentioned as a tip by the course leader the code is oriented by the solutions teached during the drl - nanodegree.\n",
    "\n",
    "### 1. Start the Environment\n",
    "Importing some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Agent and Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Agent\n",
    "For this project, agents architecture of [Project 2](https://github.com/Leschek-Kop/DDPG-Continuous-Control-Reacher) is used and modified (code in `ddpg_agent.py`). In `maddpg_agents.py` a wrapper as a multi agent handler is defined to solve the multi agent reinforcement learning task (MARL).\n",
    "\n",
    "The main model behind the ddpg is an actor - critic architecture. You can define the agents NN by setting the input_size, hidden_layers and output_size. The definition for hidden_layers, e.g. `hidden_layers=[10, 12]`, will be interpreted as two (2) hidden layers with 10 respectively 12 neurons (a default `hidden_layers=[256, 128]` is set if given hidden_layers=None).\n",
    "\n",
    "A fully connected forward network with a relu activation function between the layers for Actor and Critic is used.\n",
    "\n",
    "The Actor - Network has a tanh activation function for the output layer. This correlates with the requirements of the environment for the action vector (must be a number between `-1` and `1`).\n",
    "\n",
    "The DDPG - Agents are defined with target networks and soft-update for actor and critic networks. The `**kwargs` are used to overwrite agents defaults for:\n",
    " + BATCH_SIZE = 128\n",
    " + BUFFER_SIZE = int(1e5)\n",
    " + GAMMA = 0.99\n",
    " + LR_ACTOR = 1e-3\n",
    " + LR_CRITIC = 1e-3\n",
    " + TAU = 1e-3\n",
    " + SIGMA = 0.2\n",
    "\n",
    "If cuda is available, the agent will try to prefer cuda over cpu for training. Parameter for soft update is given by the hyperparameter `TAU`. The Exploration-Exploitation problem is addressed by the Ornstein-Uhlenbeck process (for additional action noise). Parameter `SIGMA` is used to weight the additional noise.\n",
    "\n",
    "As optimizer the SGD - Adam optimizer (with momentum) is used for better performance.\n",
    "The learning algorithm to train the MADDPG Agents is realized with an Ornstein-Uhlenbeck process to define the exploration-exploitation during training. \n",
    "\n",
    "As an option a shared replay buffer can be set with an additional key-word-argument `MEMORY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maddpg_agents import MultiAgents\n",
    "from ddpg_agent import Agent as DDPGAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Results\n",
    "A function to plot the scores (in blue) and optional average scores (in red) over episodes with (inline) matplotlib is provided.\n",
    "\n",
    "The multi agent ddpg-training function (multi_ddpg) is set with parameters to define the training and the monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, scores_avg=None):\n",
    "    \"\"\"Plot scores ans average (option).\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        scores (array): List of Rewards per Episode\n",
    "        scores_all_avg (array): List of moving average of reward per Episode\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    if not scores_avg == None:\n",
    "        plt.plot(np.arange(len(scores)), scores, label='Scores', color='blue')\n",
    "        plt.plot(np.arange(len(scores_avg)), scores_avg, label='Average', color='red')\n",
    "        # show a legend on the plot\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.show()\n",
    "\n",
    "def multi_ddpg(env, brain_name, maddpg, num_agents, n_episodes=10000, queue=100,\n",
    "               print_every=100, stop_solved=0.5, chkpoint_name='checkpoint'):\n",
    "    \"\"\"Train DDPG Agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            env (object): Reacher Environment\n",
    "            brain_name (object): Env brain name\n",
    "            maddpg (object): MADDPG Object, wrapper to handle multiple agents\n",
    "            num_agents (int): Number of agent in environment\n",
    "            n_episodes (int): Number of episodes\n",
    "            queue (int): window for monitoring purposes. Defines the rewards average\n",
    "            print_every (int): parameter for fixed print information in terminal\n",
    "            stop_solved (float): mean reward over specific windows size to achieve, defined by parameter queue\n",
    "            chkpoint_name (string): suffix for checkpoint names for critic_* and actor_* checkpoint\n",
    "\n",
    "        Return\n",
    "        ======\n",
    "            scores_all (array): List of Rewards per Episode\n",
    "            scores_all_avg (array): List of moving average of reward per Episode over window size defined by parameter \"queue\"\n",
    "        \"\"\"\n",
    "    scores_window = deque(maxlen=queue)\n",
    "    scores_all = []\n",
    "    scores_all_avg = []\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        maddpg.reset()  # reset agents exploration weights\n",
    "        states = env_info.vector_observations  # get the current state\n",
    "        scores = np.zeros(num_agents)  # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = maddpg.act(states, add_noise=True)  # select an action for each agent, clipped between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]  # send all actions to the environment\n",
    "            next_states = env_info.vector_observations  # get env next states\n",
    "            rewards = env_info.rewards  # get reward for each agent\n",
    "            dones = env_info.local_done  # see if episode finished for any agent\n",
    "            maddpg.step(states, actions, rewards, next_states, dones)  # Save and learn\n",
    "            scores += rewards  # update the score for each agent\n",
    "            states = next_states  # roll over states to next time step\n",
    "            if np.any(dones):  # exit loop if episode finished for any agent\n",
    "                break\n",
    "        scores_window.append(np.amax(scores))\n",
    "        scores_all.append(np.amax(scores))\n",
    "        scores_all_avg.append(np.mean(scores_window))\n",
    "\n",
    "        \"\"\"Print progress\"\"\"\n",
    "        print('\\rEpisode {}\\tReward: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, np.amax(scores),\n",
    "                                                                           np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            \"\"\"Print progress and keep it in console log\"\"\"\n",
    "            print('\\rEpisode {}\\tReward: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, np.amax(scores),\n",
    "                                                                               np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= stop_solved:\n",
    "            \"\"\"Goal reached, save weights and quit.\"\"\"\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                         np.mean(scores_window)))\n",
    "            maddpg.save_chkpoints(chkpoint_name)\n",
    "            break\n",
    "\n",
    "    return scores_all, scores_all_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Agent\n",
    "\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```\n",
    "\n",
    "After the environment is loaded environments **Brain** has to be defined. Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of `+0.1`. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of `-0.01`. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of `8` variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "In the last code cell some additional information about the environment are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Start Training:\n",
    "\n",
    "The next code cell will define an  MADDPG-Agent and the training environment. Two agents are defined with:\n",
    "\n",
    " + hidden_layers_actor = [512, 256, 64]      # 3 Hidden layers with 512, 256 and 64 neurons.\n",
    " + hidden_layers_critic = [512, 256, 64]     # 3 Hidden layers with 512, 256 and 64 neurons.\n",
    " + GAMMA = 0.99\n",
    " + LR_ACTOR = 1e-4\n",
    " + LR_CRITIC = 1e-4\n",
    " + TAU = 1e-3\n",
    " + SIGMA = 0.1\n",
    "\n",
    "For the MADDPG a `MEMORY_MODE = 2` is set for a shered repley buffer. Parameter for the buffer are:\n",
    "\n",
    " + BATCH_SIZE = 256\n",
    " + BUFFER_SIZE = int(1e6)\n",
    "\n",
    "The training is set to max. 10000 episodes (eps) and will stop if the average reward will hit `>= 0.5` (stop_solved).\n",
    "The task is episodic, and in order to solve the environment, the agent must get an average score of `0.5` over `100` consecutive episodes.\n",
    "\n",
    "The task is a collaboration and competition one. Both agents trying to max. their own reward by keeping the ball as long as possible in the game.\n",
    "\n",
    "Results for the defined agent are plotted below the code cell as well as the number of episodes needed to solve the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Start training with '2' agents ####\n",
      "Episode 100\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 200\tReward: -0.00\tAverage Score: 0.01\n",
      "Episode 300\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 400\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 500\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 600\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 700\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 800\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 900\tReward: -0.00\tAverage Score: 0.01\n",
      "Episode 1000\tReward: -0.00\tAverage Score: 0.01\n",
      "Episode 1100\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 1200\tReward: -0.00\tAverage Score: 0.01\n",
      "Episode 1300\tReward: -0.00\tAverage Score: 0.02\n",
      "Episode 1400\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 1500\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 1600\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 1700\tReward: -0.00\tAverage Score: 0.00\n",
      "Episode 1800\tReward: -0.00\tAverage Score: 0.02\n",
      "Episode 1900\tReward: 0.05\tAverage Score: 0.022\n",
      "Episode 2000\tReward: 0.05\tAverage Score: 0.044\n",
      "Episode 2100\tReward: 0.05\tAverage Score: 0.055\n",
      "Episode 2200\tReward: 0.05\tAverage Score: 0.044\n",
      "Episode 2300\tReward: -0.00\tAverage Score: 0.04\n",
      "Episode 2400\tReward: 0.05\tAverage Score: 0.066\n",
      "Episode 2500\tReward: 0.05\tAverage Score: 0.077\n",
      "Episode 2600\tReward: 0.05\tAverage Score: 0.088\n",
      "Episode 2700\tReward: -0.00\tAverage Score: 0.08\n",
      "Episode 2800\tReward: 0.05\tAverage Score: 0.088\n",
      "Episode 2900\tReward: 0.05\tAverage Score: 0.088\n",
      "Episode 3000\tReward: 0.05\tAverage Score: 0.088\n",
      "Episode 3100\tReward: 0.05\tAverage Score: 0.100\n",
      "Episode 3200\tReward: -0.00\tAverage Score: 0.09\n",
      "Episode 3300\tReward: 0.20\tAverage Score: 0.109\n",
      "Episode 3400\tReward: 0.05\tAverage Score: 0.110\n",
      "Episode 3500\tReward: 0.05\tAverage Score: 0.111\n",
      "Episode 3600\tReward: 0.05\tAverage Score: 0.111\n",
      "Episode 3700\tReward: 0.05\tAverage Score: 0.132\n",
      "Episode 3800\tReward: 0.10\tAverage Score: 0.111\n",
      "Episode 3900\tReward: 0.10\tAverage Score: 0.111\n",
      "Episode 4000\tReward: 0.05\tAverage Score: 0.122\n",
      "Episode 4100\tReward: 0.05\tAverage Score: 0.111\n",
      "Episode 4200\tReward: 0.15\tAverage Score: 0.122\n",
      "Episode 4300\tReward: 0.05\tAverage Score: 0.122\n",
      "Episode 4400\tReward: -0.00\tAverage Score: 0.12\n",
      "Episode 4500\tReward: 0.05\tAverage Score: 0.132\n",
      "Episode 4600\tReward: 0.15\tAverage Score: 0.133\n",
      "Episode 4700\tReward: 0.05\tAverage Score: 0.134\n",
      "Episode 4800\tReward: 0.10\tAverage Score: 0.13\n",
      "Episode 4900\tReward: 0.05\tAverage Score: 0.144\n",
      "Episode 5000\tReward: 0.15\tAverage Score: 0.133\n",
      "Episode 5100\tReward: 0.10\tAverage Score: 0.17\n",
      "Episode 5200\tReward: 0.05\tAverage Score: 0.18\n",
      "Episode 5300\tReward: 0.15\tAverage Score: 0.18\n",
      "Episode 5400\tReward: 0.15\tAverage Score: 0.198\n",
      "Episode 5500\tReward: 0.40\tAverage Score: 0.23\n",
      "Episode 5600\tReward: 0.05\tAverage Score: 0.222\n",
      "Episode 5700\tReward: 0.15\tAverage Score: 0.233\n",
      "Episode 5800\tReward: 0.25\tAverage Score: 0.25\n",
      "Episode 5900\tReward: 0.05\tAverage Score: 0.28\n",
      "Episode 6000\tReward: 0.10\tAverage Score: 0.34\n",
      "Episode 6095\tReward: 1.80\tAverage Score: 0.512\n",
      "Environment solved in 6095 episodes!\tAverage Score: 0.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_ = 0\n",
    "env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "states = env_info.vector_observations  # test set of environments states\n",
    "num_agents = len(env_info.agents)\n",
    "state_size = states.shape[1]  # num states\n",
    "action_size = brain.vector_action_space_size  # num actions\n",
    "state_size = state_size * num_agents\n",
    "hidden_layers_actor = [256, 128, 64]\n",
    "hidden_layers_critic = [256, 128, 64]\n",
    "eps = 10000\n",
    "date = 20210704\n",
    "suffix = 'SOLVED'\n",
    "print(\"\\n#### Start training with '{}' agents ####\".format(num_agents), end=\"\\n\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Define Agents\n",
    "ddpg_config_list = [\n",
    "    {\"state_size\": state_size, \"action_size\": action_size, \"seed\": seed_, \"hidden_layers_actor\": hidden_layers_actor,\n",
    "     \"hidden_layers_critic\": hidden_layers_critic, \"kwargs\": {\"BATCH_SIZE\": 256, \"BUFFER_SIZE\": int(1e6), \"GAMMA\": 0.99,\n",
    "                                                              \"LR_ACTOR\": 1e-3, \"LR_CRITIC\": 1e-3, \"TAU\": 1e-3, \n",
    "                                                              \"SIGMA\": 0.1}\n",
    "    },\n",
    "    {\"state_size\": state_size, \"action_size\": action_size, \"seed\": seed_, \"hidden_layers_actor\": hidden_layers_actor,\n",
    "     \"hidden_layers_critic\": hidden_layers_critic, \"kwargs\": {\"BATCH_SIZE\": 256, \"BUFFER_SIZE\": int(1e6), \"GAMMA\": 0.99,\n",
    "                                                              \"LR_ACTOR\": 1e-3, \"LR_CRITIC\": 1e-3, \"TAU\": 1e-3, \n",
    "                                                              \"SIGMA\": 0.1}\n",
    "    }\n",
    "]\n",
    "# Define MARL settings\n",
    "maddpg_config = {\n",
    "    \"STATE_SIZE\": state_size,\n",
    "    \"ACTION_SIZE\": action_size,\n",
    "    \"NUM_AGENTS\": num_agents,\n",
    "    \"MEMORY\": [{\n",
    "        \"action_size\": action_size,\n",
    "        \"buffer_size\": int(1e6),\n",
    "        \"batch_size\": 256,\n",
    "        \"seed\": seed_\n",
    "    }],\n",
    "    \"MEMORY_MODE\": 2  # 2: Shared relay buffer\n",
    "}\n",
    "multiagents = MultiAgents(ddpg_config_list, maddpg_config)\n",
    "\n",
    "# Start training\n",
    "scores, scores_avg = multi_ddpg(env, brain_name, multiagents, num_agents=num_agents,\n",
    "                                         n_episodes=eps, queue=100, print_every=100, stop_solved=0.5,\n",
    "                                         chkpoint_name=\"checkpoint_{}_{}_{}_{}\".format(num_agents, eps, date, suffix))\n",
    "# plot the scores\n",
    "plot_scores(scores, scores_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch your trained Agent\n",
    "Use the trained weights and watch the agent acting in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.4000000059604645\n",
      "Score (max over agents) from episode 2: 0.20000000298023224\n",
      "Score (max over agents) from episode 3: 0.10000000149011612\n",
      "Score (max over agents) from episode 4: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = multiagents.act(states, add_noise=True)  # select an action for each agent, clipped between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Finish\n",
    "When finished, you can close the environment by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "\n",
    "There are a lot of improvements to make. Some of them are:\n",
    " + Add parameter noise for exploration\n",
    " + Add batch normalization to improve learn performance\n",
    " + prioritized experience replay\n",
    " + Test an marl achitecture with shared critic\n",
    " + Run a empirical case study for hyperparameter alpha (LR - learning Rate) for actor and critic and tau ( for softupdate ) to improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
